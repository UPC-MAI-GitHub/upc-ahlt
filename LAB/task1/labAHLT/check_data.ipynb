{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Baseline NERC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check if the name contains special symbols \n",
    "def check_chemical_name(some_text:str) -> str:\n",
    "    \"\"\"Function which checks whether a text contains numbers, special characters \n",
    "    which are similar to chemical names.\"\"\"\n",
    "    ## naming binary molecules \n",
    "    greek_prefix = [\"hemi\",\"mono\",\n",
    "                    \"di\",\"tri\",\n",
    "                    \"tetra\",\"penta\",\n",
    "                    \"hexa\",\"hepta\",\n",
    "                    \"octa\",\"nona\",\n",
    "                    \"deca\"]\n",
    "    ## Metallic Names \n",
    "    metal_names = [\"cuprous\",\"cupric\",\n",
    "                   \"ferrous\",\"ferric\",\n",
    "                   \"mercurous\",\"mercuric\",\n",
    "                   \"stannous\",\"stannic\"]\n",
    "    ## Non-metal suffixes \n",
    "    non_metal_names = [\"ide\"]\n",
    "    ## Others\n",
    "    other_names = ['ite','ate']\n",
    "    ## Bonds \n",
    "    bonds = [\"ene\",'yne']\n",
    "    ## functional groups \n",
    "    func_groups = [\n",
    "                   \"carboxy\",\"carbamoyl\",\n",
    "                   \"chloroformyl\",\"hydroxy\",\n",
    "                   \"formyl\",\"oxo\",\"alkyl\",\n",
    "                   \"alkoxy\",\"epoxy\",\"halo\",\n",
    "                   \"amine\",\"cyano\",\"nitro\",\"nitroso\",\n",
    "                   \"azo\",\"sulpho\",\"alkyl thio\",\"mercapto\"\n",
    "                   ]\n",
    "    ## functional group suffixes \n",
    "    func_g_suffix = [\n",
    "                    \"oic\",\"amide\",\"oyl\",\"chloride\",\"acid\",\n",
    "                    \"ol\",\"al\",\"one\",\"carboxylate\",\"amine\",\"nitrile\",\n",
    "                    \"sulphonic\",\"thiol\"\n",
    "                    ]\n",
    "    ## other names \n",
    "    other_names2 = [\"meth\",\"eth\",\"methyl\",\"ethyl\",\"yl\"]\n",
    "    ## bring them all together \n",
    "    tmp_list = [greek_prefix,\n",
    "                metal_names,non_metal_names,\n",
    "                other_names,\n",
    "                bonds,\n",
    "                func_groups,\n",
    "                func_g_suffix,other_names2]\n",
    "    ## merging & making into a set\n",
    "    full_list = set(sum(tmp_list, []))\n",
    "    ## now we can check if it starts with a number ? \n",
    "    starts_with_digit = some_text[:1].isdigit()\n",
    "    ## contains numbers?\n",
    "    contains_number = any(char.isdigit() for char in some_text)\n",
    "    ## does it contain hyphens?\n",
    "    contain_hyphen = (\"-\" in some_text)\n",
    "    ## contains parenthesis\n",
    "    contain_parenthesis = (\"(\" in some_text) and (\")\" in some_text)\n",
    "    ## does it contain a comma?\n",
    "    contain_comma = (\",\" in some_text)\n",
    "    #return (full_list, starts_with_digit, contain_hyphen, contain_parenthesis,contain_comma)\n",
    "    ## iterate over our set \n",
    "    cnt = sum([1 for x in full_list if x in some_text])\n",
    "    #print(cnt)\n",
    "    ## check if the\n",
    "    val = (starts_with_digit+contains_number+contain_hyphen+contain_parenthesis+contain_comma)\n",
    "    #print(f\"Total Rules out of 5 which apply: {val}\")\n",
    "    ## contains number + hyphen + parenthesis? --> definetly a drug!\n",
    "    if contains_number and contain_hyphen and contain_parenthesis:\n",
    "        #return \"drug\"\n",
    "        print(\"drug\")\n",
    "    ## number and comma? also a drug \n",
    "    if contains_number and contain_comma:\n",
    "        #return \"drug\"\n",
    "        print(\"drug\")\n",
    "    ## if the value of our rules is greater than 3, and there is at least \n",
    "    ## one occurence of a \"chemical suffix\", return drug\n",
    "    if val > 3 and cnt>1: \n",
    "        #return \"drug\"\n",
    "        print(\"drug\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drug\n",
      "drug\n",
      "drug\n",
      "drug\n",
      "drug\n",
      "drug\n"
     ]
    }
   ],
   "source": [
    "## testing with a random drug name \n",
    "txt = \"1,2-betahydroxy-2,3-oleic acid\"\n",
    "check_chemical_name(txt)\n",
    "l = list(map(check_chemical_name, drug_name)) ## kinda sucks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.dom.minidom import parse\n",
    "import glob \n",
    "import numba \n",
    "from numba import njit\n",
    "\n",
    "## Vars\n",
    "DATA_PATH = \"./data\"\n",
    "file_list = glob.glob(DATA_PATH+\"/*/*.xml\")\n",
    "\n",
    "#@njit(nopython=False) --> Numba doesnt work with XML parser :/ \n",
    "## get more info from all the DRUGS\n",
    "def get_all_drug_names(list_of_files:list) -> dict:\n",
    "    ## parse all the trees \n",
    "    parsed_trees = list(map(parse, list_of_files))\n",
    "    ## get the elements \n",
    "    elements = list(map(lambda x: x.getElementsByTagName(\"entity\"),parsed_trees))\n",
    "    ## get their values \n",
    "    drug_type = [x[0].attributes['type'].value for x in elements if len(x)>1]\n",
    "    drug_name = [x[0].attributes['text'].value for x in elements if len(x)>1]\n",
    "    ## zip together \n",
    "    d = dict(zip(drug_name, drug_type))\n",
    "    return d\n",
    "\n",
    "d = get_all_drug_names(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "drug      363\n",
       "brand     168\n",
       "group     130\n",
       "drug_n     33\n",
       "Name: type, dtype: int64"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame.from_records(d,index=[0]).T.reset_index()\n",
    "df.columns = ['comp',\"type\"]\n",
    "#df.to_csv(\"./data/drugs.csv\")\n",
    "df.type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(694, 2)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comp</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1-methyl-4-phenyl-1,2,5,6-tetrahydropyridine</td>\n",
       "      <td>drug_n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16,16-dimethylprostaglandin E2</td>\n",
       "      <td>drug_n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18-Methoxycoronaridine</td>\n",
       "      <td>drug_n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5HT3 Antagonists</td>\n",
       "      <td>group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6MNA</td>\n",
       "      <td>drug_n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>warfarin-type anticoagulant</td>\n",
       "      <td>group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690</th>\n",
       "      <td>xanthine bronchodilators</td>\n",
       "      <td>group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691</th>\n",
       "      <td>xanthine derivatives</td>\n",
       "      <td>group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692</th>\n",
       "      <td>zidovudine</td>\n",
       "      <td>drug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693</th>\n",
       "      <td>zinc</td>\n",
       "      <td>drug</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>694 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comp    type\n",
       "0    1-methyl-4-phenyl-1,2,5,6-tetrahydropyridine  drug_n\n",
       "1                  16,16-dimethylprostaglandin E2  drug_n\n",
       "2                          18-Methoxycoronaridine  drug_n\n",
       "3                                5HT3 Antagonists   group\n",
       "4                                            6MNA  drug_n\n",
       "..                                            ...     ...\n",
       "689                   warfarin-type anticoagulant   group\n",
       "690                      xanthine bronchodilators   group\n",
       "691                          xanthine derivatives   group\n",
       "692                                    zidovudine    drug\n",
       "693                                          zinc    drug\n",
       "\n",
       "[694 rows x 2 columns]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(\"comp\",ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: ML NERC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "from os import listdir\n",
    "\n",
    "from xml.dom.minidom import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(txt):\n",
    "    offset = 0\n",
    "    tks = []\n",
    "    ## word_tokenize splits words, taking into account punctuations, numbers, etc.\n",
    "    for t in word_tokenize(txt):\n",
    "        ## keep track of the position where each token should appear, and\n",
    "        ## store that information with the token\n",
    "        offset = txt.find(t, offset)\n",
    "        tks.append((t, offset, offset+len(t)-1))\n",
    "        offset += len(t)\n",
    "\n",
    "    ## tks is a list of triples (word,start,end)\n",
    "    return tks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tag(token, spans) :\n",
    "   (_,start,end) = token\n",
    "   for (spanS,spanE,spanT) in spans :\n",
    "      if start==spanS and end<=spanE : return \"B-\"+spanT\n",
    "      elif start>=spanS and end<=spanE : return \"I-\"+spanT\n",
    "\n",
    "   return \"O\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tokens:str) -> list:\n",
    "    \"\"\"Function to extract features from the \"\"\"\n",
    "    # for each token, generate list of features and add it to the result\n",
    "    result = []\n",
    "    for k in range(0,len(tokens)):\n",
    "        tokenFeatures = []\n",
    "        t = tokens[k][0]\n",
    "        ## so we can count capitals here\n",
    "        count_caps = str(any(char.isupper() for char in t))\n",
    "        all_caps = str(t.isupper())\n",
    "        t = t.lower()\n",
    "        ## Append the actual word \n",
    "        tokenFeatures.append(\"form=\"+t)\n",
    "        ## the suffixes\n",
    "        tokenFeatures.append(\"suf5=\"+t[-5:])\n",
    "        tokenFeatures.append(\"suf4=\"+t[-4:])\n",
    "        tokenFeatures.append(\"suf3=\"+t[-3:])\n",
    "        tokenFeatures.append(\"suf2=\"+t[-2:])\n",
    "        ## get the prefixes\n",
    "        tokenFeatures.append(\"pref5=\"+t[:5])\n",
    "        tokenFeatures.append(\"pref4=\"+t[:4])\n",
    "        tokenFeatures.append(\"pref3=\"+t[:3])\n",
    "        tokenFeatures.append(\"pref2=\"+t[:2])\n",
    "        ## Are there any numbers \n",
    "        tokenFeatures.append(\"CountNum=\"+str(any(char.isdigit() for char in t)))\n",
    "        ## Any capital \n",
    "        tokenFeatures.append(\"CountCaps=\"+count_caps)\n",
    "        tokenFeatures.append(\"AllCaps=\"+all_caps)\n",
    "        ## any hyphens? \n",
    "        tokenFeatures.append(\"Hyphen=\"+str((\"-\" in t)))\n",
    "        ## parenthesis\n",
    "        tokenFeatures.append(\"Parenth=\"+str((\"(\" and \")\" in t)))\n",
    "        ## previous word - Suffix & Prefix \n",
    "        if k>0 :\n",
    "            tPrev = tokens[k-1][0]\n",
    "            tokenFeatures.append(\"Prevsuf5=\"+tPrev[-5:])\n",
    "            tokenFeatures.append(\"Prevsuf4=\"+tPrev[-4:])\n",
    "            tokenFeatures.append(\"Prevsuf3=\"+tPrev[-3:])\n",
    "            tokenFeatures.append(\"Prevsuf2=\"+tPrev[-2:])\n",
    "            ## get the prefixes\n",
    "            tokenFeatures.append(\"Prevpref5=\"+tPrev[:5])\n",
    "            tokenFeatures.append(\"Prevpref4=\"+tPrev[:4])\n",
    "            tokenFeatures.append(\"Prevpref3=\"+tPrev[:3])\n",
    "            tokenFeatures.append(\"Prevpref2=\"+tPrev[:2])\n",
    "        else :\n",
    "            tokenFeatures.append(\"BoS\")\n",
    "\n",
    "        ## The next word - Suffix + Prefix\n",
    "        if k<len(tokens)-1 :\n",
    "            tNext = tokens[k+1][0]\n",
    "            tokenFeatures.append(\"Nextsuf5=\"+tNext[-5:])\n",
    "            tokenFeatures.append(\"Nextsuf4=\"+tNext[-4:])\n",
    "            tokenFeatures.append(\"Nextsuf3=\"+tNext[-3:])\n",
    "            tokenFeatures.append(\"Nextsuf2=\"+tNext[-2:])\n",
    "            ## get the prefixes\n",
    "            tokenFeatures.append(\"Nextpref5=\"+tNext[:5])\n",
    "            tokenFeatures.append(\"Nextpref4=\"+tNext[:4])\n",
    "            tokenFeatures.append(\"Nextpref3=\"+tNext[:3])\n",
    "            tokenFeatures.append(\"Nextpref2=\"+tNext[:2])\n",
    "        else:\n",
    "            tokenFeatures.append(\"EoS\")\n",
    "        ## Next Next Word (Plus 2)\n",
    "        if k<len(tokens)-2 :\n",
    "            tNextNext = tokens[k+2][0]\n",
    "            tokenFeatures.append(\"NxtNextsuf5=\"+tNextNext[-5:])\n",
    "            tokenFeatures.append(\"NxtNextsuf4=\"+tNextNext[-4:])\n",
    "            tokenFeatures.append(\"NxtNextsuf3=\"+tNextNext[-3:])\n",
    "            tokenFeatures.append(\"NxtNextsuf2=\"+tNextNext[-2:])\n",
    "            ## get the prefixes\n",
    "            tokenFeatures.append(\"NxtNextpref5=\"+tNextNext[:5])\n",
    "            tokenFeatures.append(\"NxtNextpref4=\"+tNextNext[:4])\n",
    "            tokenFeatures.append(\"NxtNextpref3=\"+tNextNext[:3])\n",
    "            tokenFeatures.append(\"NxtNextpref2=\"+tNextNext[:2])\n",
    "        ## Previous Previous (Minus 2)\n",
    "        if k>1:\n",
    "            tPrevPrev = tokens[k-2][0]\n",
    "            tokenFeatures.append(\"PrePrevsuf5=\"+tPrevPrev[-5:])\n",
    "            tokenFeatures.append(\"PrePrevsuf4=\"+tPrevPrev[-4:])\n",
    "            tokenFeatures.append(\"PrePrevsuf3=\"+tPrevPrev[-3:])\n",
    "            tokenFeatures.append(\"PrePrevsuf2=\"+tPrevPrev[-2:])\n",
    "            ## get the prefixes\n",
    "            tokenFeatures.append(\"PrevPrevpref5=\"+tPrevPrev[:5])\n",
    "            tokenFeatures.append(\"PrevPrevpref4=\"+tPrevPrev[:4])\n",
    "            tokenFeatures.append(\"PrevPrevpref3=\"+tPrevPrev[:3])\n",
    "            tokenFeatures.append(\"PrevPrevpref2=\"+tPrevPrev[:2])\n",
    "        \n",
    "        result.append(tokenFeatures)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"/Users/Eric/Documents/Uni/Msc/Courses/Sem2/AHLT/LAB/task1/labAHLT/data/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Milk, milk products, and calcium-rich foods or drugs may impair the absorption of EMCYT.\n",
      "['form=milk', 'suf5=milk', 'suf4=milk', 'suf3=ilk', 'suf2=lk', 'pref5=milk', 'pref4=milk', 'pref3=mil', 'pref2=mi', 'CountNum=False', 'CountCaps=True', 'AllCaps=False', 'Hyphen=False', 'Parenth=False', 'BoS', 'Nextsuf5=,', 'Nextsuf4=,', 'Nextsuf3=,', 'Nextsuf2=,', 'Nextpref5=,', 'Nextpref4=,', 'Nextpref3=,', 'Nextpref2=,', 'NxtNextsuf5=milk', 'NxtNextsuf4=milk', 'NxtNextsuf3=ilk', 'NxtNextsuf2=lk', 'NxtNextpref5=milk', 'NxtNextpref4=milk', 'NxtNextpref3=mil', 'NxtNextpref2=mi']\n",
      "['form=,', 'suf5=,', 'suf4=,', 'suf3=,', 'suf2=,', 'pref5=,', 'pref4=,', 'pref3=,', 'pref2=,', 'CountNum=False', 'CountCaps=False', 'AllCaps=False', 'Hyphen=False', 'Parenth=False', 'Prevsuf5=Milk', 'Prevsuf4=Milk', 'Prevsuf3=ilk', 'Prevsuf2=lk', 'Prevpref5=Milk', 'Prevpref4=Milk', 'Prevpref3=Mil', 'Prevpref2=Mi', 'Nextsuf5=milk', 'Nextsuf4=milk', 'Nextsuf3=ilk', 'Nextsuf2=lk', 'Nextpref5=milk', 'Nextpref4=milk', 'Nextpref3=mil', 'Nextpref2=mi', 'NxtNextsuf5=ducts', 'NxtNextsuf4=ucts', 'NxtNextsuf3=cts', 'NxtNextsuf2=ts', 'NxtNextpref5=produ', 'NxtNextpref4=prod', 'NxtNextpref3=pro', 'NxtNextpref2=pr']\n",
      "['form=milk', 'suf5=milk', 'suf4=milk', 'suf3=ilk', 'suf2=lk', 'pref5=milk', 'pref4=milk', 'pref3=mil', 'pref2=mi', 'CountNum=False', 'CountCaps=False', 'AllCaps=False', 'Hyphen=False', 'Parenth=False', 'Prevsuf5=,', 'Prevsuf4=,', 'Prevsuf3=,', 'Prevsuf2=,', 'Prevpref5=,', 'Prevpref4=,', 'Prevpref3=,', 'Prevpref2=,', 'Nextsuf5=ducts', 'Nextsuf4=ucts', 'Nextsuf3=cts', 'Nextsuf2=ts', 'Nextpref5=produ', 'Nextpref4=prod', 'Nextpref3=pro', 'Nextpref2=pr', 'NxtNextsuf5=,', 'NxtNextsuf4=,', 'NxtNextsuf3=,', 'NxtNextsuf2=,', 'NxtNextpref5=,', 'NxtNextpref4=,', 'NxtNextpref3=,', 'NxtNextpref2=,', 'PrePrevsuf5=Milk', 'PrePrevsuf4=Milk', 'PrePrevsuf3=ilk', 'PrePrevsuf2=lk', 'PrevPrevpref5=Milk', 'PrevPrevpref4=Milk', 'PrevPrevpref3=Mil', 'PrevPrevpref2=Mi']\n",
      "['form=products', 'suf5=ducts', 'suf4=ucts', 'suf3=cts', 'suf2=ts', 'pref5=produ', 'pref4=prod', 'pref3=pro', 'pref2=pr', 'CountNum=False', 'CountCaps=False', 'AllCaps=False', 'Hyphen=False', 'Parenth=False', 'Prevsuf5=milk', 'Prevsuf4=milk', 'Prevsuf3=ilk', 'Prevsuf2=lk', 'Prevpref5=milk', 'Prevpref4=milk', 'Prevpref3=mil', 'Prevpref2=mi', 'Nextsuf5=,', 'Nextsuf4=,', 'Nextsuf3=,', 'Nextsuf2=,', 'Nextpref5=,', 'Nextpref4=,', 'Nextpref3=,', 'Nextpref2=,', 'NxtNextsuf5=and', 'NxtNextsuf4=and', 'NxtNextsuf3=and', 'NxtNextsuf2=nd', 'NxtNextpref5=and', 'NxtNextpref4=and', 'NxtNextpref3=and', 'NxtNextpref2=an', 'PrePrevsuf5=,', 'PrePrevsuf4=,', 'PrePrevsuf3=,', 'PrePrevsuf2=,', 'PrevPrevpref5=,', 'PrevPrevpref4=,', 'PrevPrevpref3=,', 'PrevPrevpref2=,']\n",
      "['form=,', 'suf5=,', 'suf4=,', 'suf3=,', 'suf2=,', 'pref5=,', 'pref4=,', 'pref3=,', 'pref2=,', 'CountNum=False', 'CountCaps=False', 'AllCaps=False', 'Hyphen=False', 'Parenth=False', 'Prevsuf5=ducts', 'Prevsuf4=ucts', 'Prevsuf3=cts', 'Prevsuf2=ts', 'Prevpref5=produ', 'Prevpref4=prod', 'Prevpref3=pro', 'Prevpref2=pr', 'Nextsuf5=and', 'Nextsuf4=and', 'Nextsuf3=and', 'Nextsuf2=nd', 'Nextpref5=and', 'Nextpref4=and', 'Nextpref3=and', 'Nextpref2=an', 'NxtNextsuf5=-rich', 'NxtNextsuf4=rich', 'NxtNextsuf3=ich', 'NxtNextsuf2=ch', 'NxtNextpref5=calci', 'NxtNextpref4=calc', 'NxtNextpref3=cal', 'NxtNextpref2=ca', 'PrePrevsuf5=milk', 'PrePrevsuf4=milk', 'PrePrevsuf3=ilk', 'PrePrevsuf2=lk', 'PrevPrevpref5=milk', 'PrevPrevpref4=milk', 'PrevPrevpref3=mil', 'PrevPrevpref2=mi']\n",
      "['form=and', 'suf5=and', 'suf4=and', 'suf3=and', 'suf2=nd', 'pref5=and', 'pref4=and', 'pref3=and', 'pref2=an', 'CountNum=False', 'CountCaps=False', 'AllCaps=False', 'Hyphen=False', 'Parenth=False', 'Prevsuf5=,', 'Prevsuf4=,', 'Prevsuf3=,', 'Prevsuf2=,', 'Prevpref5=,', 'Prevpref4=,', 'Prevpref3=,', 'Prevpref2=,', 'Nextsuf5=-rich', 'Nextsuf4=rich', 'Nextsuf3=ich', 'Nextsuf2=ch', 'Nextpref5=calci', 'Nextpref4=calc', 'Nextpref3=cal', 'Nextpref2=ca', 'NxtNextsuf5=foods', 'NxtNextsuf4=oods', 'NxtNextsuf3=ods', 'NxtNextsuf2=ds', 'NxtNextpref5=foods', 'NxtNextpref4=food', 'NxtNextpref3=foo', 'NxtNextpref2=fo', 'PrePrevsuf5=ducts', 'PrePrevsuf4=ucts', 'PrePrevsuf3=cts', 'PrePrevsuf2=ts', 'PrevPrevpref5=produ', 'PrevPrevpref4=prod', 'PrevPrevpref3=pro', 'PrevPrevpref2=pr']\n",
      "['form=calcium-rich', 'suf5=-rich', 'suf4=rich', 'suf3=ich', 'suf2=ch', 'pref5=calci', 'pref4=calc', 'pref3=cal', 'pref2=ca', 'CountNum=False', 'CountCaps=False', 'AllCaps=False', 'Hyphen=True', 'Parenth=False', 'Prevsuf5=and', 'Prevsuf4=and', 'Prevsuf3=and', 'Prevsuf2=nd', 'Prevpref5=and', 'Prevpref4=and', 'Prevpref3=and', 'Prevpref2=an', 'Nextsuf5=foods', 'Nextsuf4=oods', 'Nextsuf3=ods', 'Nextsuf2=ds', 'Nextpref5=foods', 'Nextpref4=food', 'Nextpref3=foo', 'Nextpref2=fo', 'NxtNextsuf5=or', 'NxtNextsuf4=or', 'NxtNextsuf3=or', 'NxtNextsuf2=or', 'NxtNextpref5=or', 'NxtNextpref4=or', 'NxtNextpref3=or', 'NxtNextpref2=or', 'PrePrevsuf5=,', 'PrePrevsuf4=,', 'PrePrevsuf3=,', 'PrePrevsuf2=,', 'PrevPrevpref5=,', 'PrevPrevpref4=,', 'PrevPrevpref3=,', 'PrevPrevpref2=,']\n",
      "['form=foods', 'suf5=foods', 'suf4=oods', 'suf3=ods', 'suf2=ds', 'pref5=foods', 'pref4=food', 'pref3=foo', 'pref2=fo', 'CountNum=False', 'CountCaps=False', 'AllCaps=False', 'Hyphen=False', 'Parenth=False', 'Prevsuf5=-rich', 'Prevsuf4=rich', 'Prevsuf3=ich', 'Prevsuf2=ch', 'Prevpref5=calci', 'Prevpref4=calc', 'Prevpref3=cal', 'Prevpref2=ca', 'Nextsuf5=or', 'Nextsuf4=or', 'Nextsuf3=or', 'Nextsuf2=or', 'Nextpref5=or', 'Nextpref4=or', 'Nextpref3=or', 'Nextpref2=or', 'NxtNextsuf5=drugs', 'NxtNextsuf4=rugs', 'NxtNextsuf3=ugs', 'NxtNextsuf2=gs', 'NxtNextpref5=drugs', 'NxtNextpref4=drug', 'NxtNextpref3=dru', 'NxtNextpref2=dr', 'PrePrevsuf5=and', 'PrePrevsuf4=and', 'PrePrevsuf3=and', 'PrePrevsuf2=nd', 'PrevPrevpref5=and', 'PrevPrevpref4=and', 'PrevPrevpref3=and', 'PrevPrevpref2=an']\n",
      "['form=or', 'suf5=or', 'suf4=or', 'suf3=or', 'suf2=or', 'pref5=or', 'pref4=or', 'pref3=or', 'pref2=or', 'CountNum=False', 'CountCaps=False', 'AllCaps=False', 'Hyphen=False', 'Parenth=False', 'Prevsuf5=foods', 'Prevsuf4=oods', 'Prevsuf3=ods', 'Prevsuf2=ds', 'Prevpref5=foods', 'Prevpref4=food', 'Prevpref3=foo', 'Prevpref2=fo', 'Nextsuf5=drugs', 'Nextsuf4=rugs', 'Nextsuf3=ugs', 'Nextsuf2=gs', 'Nextpref5=drugs', 'Nextpref4=drug', 'Nextpref3=dru', 'Nextpref2=dr', 'NxtNextsuf5=may', 'NxtNextsuf4=may', 'NxtNextsuf3=may', 'NxtNextsuf2=ay', 'NxtNextpref5=may', 'NxtNextpref4=may', 'NxtNextpref3=may', 'NxtNextpref2=ma', 'PrePrevsuf5=-rich', 'PrePrevsuf4=rich', 'PrePrevsuf3=ich', 'PrePrevsuf2=ch', 'PrevPrevpref5=calci', 'PrevPrevpref4=calc', 'PrevPrevpref3=cal', 'PrevPrevpref2=ca']\n",
      "['form=drugs', 'suf5=drugs', 'suf4=rugs', 'suf3=ugs', 'suf2=gs', 'pref5=drugs', 'pref4=drug', 'pref3=dru', 'pref2=dr', 'CountNum=False', 'CountCaps=False', 'AllCaps=False', 'Hyphen=False', 'Parenth=False', 'Prevsuf5=or', 'Prevsuf4=or', 'Prevsuf3=or', 'Prevsuf2=or', 'Prevpref5=or', 'Prevpref4=or', 'Prevpref3=or', 'Prevpref2=or', 'Nextsuf5=may', 'Nextsuf4=may', 'Nextsuf3=may', 'Nextsuf2=ay', 'Nextpref5=may', 'Nextpref4=may', 'Nextpref3=may', 'Nextpref2=ma', 'NxtNextsuf5=mpair', 'NxtNextsuf4=pair', 'NxtNextsuf3=air', 'NxtNextsuf2=ir', 'NxtNextpref5=impai', 'NxtNextpref4=impa', 'NxtNextpref3=imp', 'NxtNextpref2=im', 'PrePrevsuf5=foods', 'PrePrevsuf4=oods', 'PrePrevsuf3=ods', 'PrePrevsuf2=ds', 'PrevPrevpref5=foods', 'PrevPrevpref4=food', 'PrevPrevpref3=foo', 'PrevPrevpref2=fo']\n",
      "['form=may', 'suf5=may', 'suf4=may', 'suf3=may', 'suf2=ay', 'pref5=may', 'pref4=may', 'pref3=may', 'pref2=ma', 'CountNum=False', 'CountCaps=False', 'AllCaps=False', 'Hyphen=False', 'Parenth=False', 'Prevsuf5=drugs', 'Prevsuf4=rugs', 'Prevsuf3=ugs', 'Prevsuf2=gs', 'Prevpref5=drugs', 'Prevpref4=drug', 'Prevpref3=dru', 'Prevpref2=dr', 'Nextsuf5=mpair', 'Nextsuf4=pair', 'Nextsuf3=air', 'Nextsuf2=ir', 'Nextpref5=impai', 'Nextpref4=impa', 'Nextpref3=imp', 'Nextpref2=im', 'NxtNextsuf5=the', 'NxtNextsuf4=the', 'NxtNextsuf3=the', 'NxtNextsuf2=he', 'NxtNextpref5=the', 'NxtNextpref4=the', 'NxtNextpref3=the', 'NxtNextpref2=th', 'PrePrevsuf5=or', 'PrePrevsuf4=or', 'PrePrevsuf3=or', 'PrePrevsuf2=or', 'PrevPrevpref5=or', 'PrevPrevpref4=or', 'PrevPrevpref3=or', 'PrevPrevpref2=or']\n",
      "['form=impair', 'suf5=mpair', 'suf4=pair', 'suf3=air', 'suf2=ir', 'pref5=impai', 'pref4=impa', 'pref3=imp', 'pref2=im', 'CountNum=False', 'CountCaps=False', 'AllCaps=False', 'Hyphen=False', 'Parenth=False', 'Prevsuf5=may', 'Prevsuf4=may', 'Prevsuf3=may', 'Prevsuf2=ay', 'Prevpref5=may', 'Prevpref4=may', 'Prevpref3=may', 'Prevpref2=ma', 'Nextsuf5=the', 'Nextsuf4=the', 'Nextsuf3=the', 'Nextsuf2=he', 'Nextpref5=the', 'Nextpref4=the', 'Nextpref3=the', 'Nextpref2=th', 'NxtNextsuf5=ption', 'NxtNextsuf4=tion', 'NxtNextsuf3=ion', 'NxtNextsuf2=on', 'NxtNextpref5=absor', 'NxtNextpref4=abso', 'NxtNextpref3=abs', 'NxtNextpref2=ab', 'PrePrevsuf5=drugs', 'PrePrevsuf4=rugs', 'PrePrevsuf3=ugs', 'PrePrevsuf2=gs', 'PrevPrevpref5=drugs', 'PrevPrevpref4=drug', 'PrevPrevpref3=dru', 'PrevPrevpref2=dr']\n",
      "['form=the', 'suf5=the', 'suf4=the', 'suf3=the', 'suf2=he', 'pref5=the', 'pref4=the', 'pref3=the', 'pref2=th', 'CountNum=False', 'CountCaps=False', 'AllCaps=False', 'Hyphen=False', 'Parenth=False', 'Prevsuf5=mpair', 'Prevsuf4=pair', 'Prevsuf3=air', 'Prevsuf2=ir', 'Prevpref5=impai', 'Prevpref4=impa', 'Prevpref3=imp', 'Prevpref2=im', 'Nextsuf5=ption', 'Nextsuf4=tion', 'Nextsuf3=ion', 'Nextsuf2=on', 'Nextpref5=absor', 'Nextpref4=abso', 'Nextpref3=abs', 'Nextpref2=ab', 'NxtNextsuf5=of', 'NxtNextsuf4=of', 'NxtNextsuf3=of', 'NxtNextsuf2=of', 'NxtNextpref5=of', 'NxtNextpref4=of', 'NxtNextpref3=of', 'NxtNextpref2=of', 'PrePrevsuf5=may', 'PrePrevsuf4=may', 'PrePrevsuf3=may', 'PrePrevsuf2=ay', 'PrevPrevpref5=may', 'PrevPrevpref4=may', 'PrevPrevpref3=may', 'PrevPrevpref2=ma']\n",
      "['form=absorption', 'suf5=ption', 'suf4=tion', 'suf3=ion', 'suf2=on', 'pref5=absor', 'pref4=abso', 'pref3=abs', 'pref2=ab', 'CountNum=False', 'CountCaps=False', 'AllCaps=False', 'Hyphen=False', 'Parenth=False', 'Prevsuf5=the', 'Prevsuf4=the', 'Prevsuf3=the', 'Prevsuf2=he', 'Prevpref5=the', 'Prevpref4=the', 'Prevpref3=the', 'Prevpref2=th', 'Nextsuf5=of', 'Nextsuf4=of', 'Nextsuf3=of', 'Nextsuf2=of', 'Nextpref5=of', 'Nextpref4=of', 'Nextpref3=of', 'Nextpref2=of', 'NxtNextsuf5=EMCYT', 'NxtNextsuf4=MCYT', 'NxtNextsuf3=CYT', 'NxtNextsuf2=YT', 'NxtNextpref5=EMCYT', 'NxtNextpref4=EMCY', 'NxtNextpref3=EMC', 'NxtNextpref2=EM', 'PrePrevsuf5=mpair', 'PrePrevsuf4=pair', 'PrePrevsuf3=air', 'PrePrevsuf2=ir', 'PrevPrevpref5=impai', 'PrevPrevpref4=impa', 'PrevPrevpref3=imp', 'PrevPrevpref2=im']\n",
      "['form=of', 'suf5=of', 'suf4=of', 'suf3=of', 'suf2=of', 'pref5=of', 'pref4=of', 'pref3=of', 'pref2=of', 'CountNum=False', 'CountCaps=False', 'AllCaps=False', 'Hyphen=False', 'Parenth=False', 'Prevsuf5=ption', 'Prevsuf4=tion', 'Prevsuf3=ion', 'Prevsuf2=on', 'Prevpref5=absor', 'Prevpref4=abso', 'Prevpref3=abs', 'Prevpref2=ab', 'Nextsuf5=EMCYT', 'Nextsuf4=MCYT', 'Nextsuf3=CYT', 'Nextsuf2=YT', 'Nextpref5=EMCYT', 'Nextpref4=EMCY', 'Nextpref3=EMC', 'Nextpref2=EM', 'NxtNextsuf5=.', 'NxtNextsuf4=.', 'NxtNextsuf3=.', 'NxtNextsuf2=.', 'NxtNextpref5=.', 'NxtNextpref4=.', 'NxtNextpref3=.', 'NxtNextpref2=.', 'PrePrevsuf5=the', 'PrePrevsuf4=the', 'PrePrevsuf3=the', 'PrePrevsuf2=he', 'PrevPrevpref5=the', 'PrevPrevpref4=the', 'PrevPrevpref3=the', 'PrevPrevpref2=th']\n",
      "['form=emcyt', 'suf5=emcyt', 'suf4=mcyt', 'suf3=cyt', 'suf2=yt', 'pref5=emcyt', 'pref4=emcy', 'pref3=emc', 'pref2=em', 'CountNum=False', 'CountCaps=True', 'AllCaps=True', 'Hyphen=False', 'Parenth=False', 'Prevsuf5=of', 'Prevsuf4=of', 'Prevsuf3=of', 'Prevsuf2=of', 'Prevpref5=of', 'Prevpref4=of', 'Prevpref3=of', 'Prevpref2=of', 'Nextsuf5=.', 'Nextsuf4=.', 'Nextsuf3=.', 'Nextsuf2=.', 'Nextpref5=.', 'Nextpref4=.', 'Nextpref3=.', 'Nextpref2=.', 'PrePrevsuf5=ption', 'PrePrevsuf4=tion', 'PrePrevsuf3=ion', 'PrePrevsuf2=on', 'PrevPrevpref5=absor', 'PrevPrevpref4=abso', 'PrevPrevpref3=abs', 'PrevPrevpref2=ab']\n",
      "['form=.', 'suf5=.', 'suf4=.', 'suf3=.', 'suf2=.', 'pref5=.', 'pref4=.', 'pref3=.', 'pref2=.', 'CountNum=False', 'CountCaps=False', 'AllCaps=False', 'Hyphen=False', 'Parenth=False', 'Prevsuf5=EMCYT', 'Prevsuf4=MCYT', 'Prevsuf3=CYT', 'Prevsuf2=YT', 'Prevpref5=EMCYT', 'Prevpref4=EMCY', 'Prevpref3=EMC', 'Prevpref2=EM', 'EoS', 'PrePrevsuf5=of', 'PrePrevsuf4=of', 'PrePrevsuf3=of', 'PrePrevsuf2=of', 'PrevPrevpref5=of', 'PrevPrevpref4=of', 'PrevPrevpref3=of', 'PrevPrevpref2=of']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## \"MAIN\"\n",
    "# process each file in directory\n",
    "for f in listdir(datadir)[:1] :\n",
    "   \n",
    "   # parse XML file, obtaining a DOM tree\n",
    "   tree = parse(datadir+\"/\"+f)\n",
    "   \n",
    "   # process each sentence in the file\n",
    "   sentences = tree.getElementsByTagName(\"sentence\")\n",
    "   for s in sentences :\n",
    "      sid = s.attributes[\"id\"].value   # get sentence id\n",
    "      spans = []\n",
    "      stext = s.attributes[\"text\"].value   # get sentence text\n",
    "      entities = s.getElementsByTagName(\"entity\")\n",
    "      for e in entities :\n",
    "         # for discontinuous entities, we only get the first span\n",
    "         # (will not work, but there are few of them)\n",
    "         (start,end) = e.attributes[\"charOffset\"].value.split(\";\")[0].split(\"-\")\n",
    "         typ =  e.attributes[\"type\"].value\n",
    "         spans.append((int(start),int(end),typ))\n",
    "         \n",
    "\n",
    "      # convert the sentence to a list of tokens\n",
    "      tokens = tokenize(stext)\n",
    "      # extract sentence features\n",
    "      features = extract_features(tokens)\n",
    "      print(stext)\n",
    "      [print(x) for x in features]\n",
    "      #[print(len(x)) for x in features]\n",
    "\n",
    "      # print features in format expected by crfsuite trainer\n",
    "      for i in range (0,len(tokens)) :\n",
    "         # see if the token is part of an entity\n",
    "         tag = get_tag(tokens[i], spans) \n",
    "         #print (sid, tokens[i][0], tokens[i][1], tokens[i][2], tag, \"\\t\".join(features[i]), sep='\\t')\n",
    "\n",
    "      # blank line to separate sentences\n",
    "      print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac6858c3dbc49267e902ff986705b591b9d7b57befff84fd7d814fe16c4a8e1f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('ci_covid')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
